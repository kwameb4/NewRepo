# -*- coding: utf-8 -*-
"""HW2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11gZauEEFnQcnsC5w1Ut0e9_NIbaa9yZF

### Getting started notebook for HW2
"""

import pandas as pd
import numpy as np

#https://www.kaggle.com/datasets/andrewmvd/sp-500-stocks?select=sp500_stocks.csv
all_stocks=pd.read_csv("sp500_stocks.csv")
all_stocks['Symbol'].unique()

#OLDER DATA _ DO NOT USE
#https://www.kaggle.com/datasets/camnugent/sandp500
# all_stocks=pd.read_csv("all_stocks_5yr.csv").rename(columns={'Name':'Symbol'})
# all_stocks['Symbol'].unique()

#likely more than 500 because of index changes
len(all_stocks['Symbol'].unique())

sector=pd.read_csv("sp500_companies.csv")

# OLD - DO NOT USE
# sector=pd.read_csv("sectors.csv")[['Ticker','Sector']].rename(columns={'Ticker':'Symbol'})
sector

all_stocks=all_stocks.merge(sector[['Symbol','Sector']],how='left',on='Symbol')
all_stocks

all_stocks['Sector'].unique()

#we sill have some missing sectors - figure out what to do here
all_stocks[all_stocks['Sector'].isnull()]['Symbol'].unique()

all_stocks[all_stocks['Symbol']=='GOOG']

# all_stocks.loc[all_stocks['Sector'].isnull(),'Sector']='UNKNOWN'

# YOU MAY NEED TO UPDATE THIS LOGIC
all_stocks.loc[all_stocks['Symbol']=='CEG','Sector']='Utilities'
all_stocks.loc[all_stocks['Symbol']=='ELV','Sector']='Healthcare'
all_stocks.loc[all_stocks['Symbol']=='META','Sector']='Communication Services'
all_stocks.loc[all_stocks['Symbol']=='PARA','Sector']='Communication Services'
all_stocks.loc[all_stocks['Symbol']=='SBUX','Sector']='Consumer Cyclical'
all_stocks.loc[all_stocks['Symbol']=='V','Sector']='Financial Services'
all_stocks.loc[all_stocks['Symbol']=='WBD','Sector']='Communication Services'
all_stocks.loc[all_stocks['Symbol']=='WTW','Sector']='Financial Services'

all_stocks['Sector'].unique()

"""### In order to understand what factors may be driving returns we first need to calcualte returns"""

#calculate return as a log-difference
all_stocks=all_stocks.sort_values(['Symbol','Date']).reset_index(drop=True)
all_stocks['adj_close_lag1']=all_stocks[['Symbol','Date','Adj Close']].groupby(['Symbol']).shift(1)['Adj Close'].reset_index(drop=True)
all_stocks['return']=np.log(all_stocks['Adj Close']/all_stocks['adj_close_lag1'])

"""### Think about how to use the other features - DO NOT USE FEATURES FROM TODAY TO MODEL TODAY'S RETURN"""

all_stocks

def create_lagged_features(df,var):
    df[var+'_lag1']=df[['Symbol','Date',var]].groupby(['Symbol']).shift(1)[var].reset_index(drop=True)
    df[var+'_rolling5']=df[['Symbol','Date',var+'_lag1']].groupby(['Symbol'])[var+'_lag1'].rolling(5).sum().reset_index(drop=True)
    df[var+'_rolling15']=df[['Symbol','Date',var+'_lag1']].groupby(['Symbol'])[var+'_lag1'].rolling(15).sum().reset_index(drop=True)
    return df

all_stocks=create_lagged_features(all_stocks,'return')
all_stocks

all_stocks=create_lagged_features(all_stocks,'Volume')
all_stocks

all_stocks['relative_vol_1_15']=all_stocks['Volume_lag1']/all_stocks['Volume_rolling15']
all_stocks['relative_vol_5_15']=all_stocks['Volume_rolling5']/all_stocks['Volume_rolling15']

"""### Transform Sector for modeling"""

sector_counts=all_stocks['Sector'].value_counts()
sector_counts

all_stocks['Sector']

#perform frequency based encoding (usually this would only use training porttion to fit transform, but need to keep transform constant across days)
from sklearn.preprocessing import OrdinalEncoder
enc = OrdinalEncoder(categories=[list(sector_counts.index)],handle_unknown='use_encoded_value',unknown_value=-1)

all_stocks['Sector_enc']=enc.fit_transform(all_stocks[['Sector']])

"""### Let's pick a stock to see what might be driving returns for that stock based on modeling the market"""

pip install shap

import shap
import xgboost as xgb
from sklearn.model_selection import GridSearchCV, train_test_split

this_stock='WMT'
feature_list=['Sector_enc','return_lag1','return_rolling5','return_rolling15','relative_vol_1_15','relative_vol_5_15']
this_date=all_stocks.loc[all_stocks.index[-3],'Date']

this_stock, this_date

this_date1=[]
this_stock=[]
for i in range(4,13):
  
  b=i*-1
  this_date1.append(all_stocks.loc[all_stocks.index[b],'Date'])
print(this_date1)

g=this_date1[4]
g

#create a list of today's stocks EXCLUDING the one we are interested in

today_stocks=all_stocks[np.logical_and(all_stocks['Date']==this_date,all_stocks['Symbol']!=this_stock)]

#for i in this_date1:
today_stocks1=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-30',all_stocks['Symbol']!=this_stock)]

today_stocks2=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-29',all_stocks['Symbol']!=this_stock)]

today_stocks3=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-28',all_stocks['Symbol']!=this_stock)]

today_stocks4=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-25',all_stocks['Symbol']!=this_stock)]

today_stocks5=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-23',all_stocks['Symbol']!=this_stock)]

today_stocks6=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-22',all_stocks['Symbol']!=this_stock)]

today_stocks7=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-21',all_stocks['Symbol']!=this_stock)]

today_stocks8=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-18',all_stocks['Symbol']!=this_stock)]

today_stocks9=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-17',all_stocks['Symbol']!=this_stock)]

#create a train/test split for early stopping. Just using about 50 stocks
X_train, X_test, y_train, y_test = train_test_split(today_stocks[feature_list], today_stocks['return'], test_size=0.1, random_state=42)

X_train1, X_test1, y_train1, y_test1 = train_test_split(today_stocks1[feature_list], today_stocks1['return'], test_size=0.1, random_state=42)

X_train2, X_test2, y_train2, y_test2 = train_test_split(today_stocks2[feature_list], today_stocks2['return'], test_size=0.1, random_state=42)

X_train3, X_test3, y_train3, y_test3 = train_test_split(today_stocks3[feature_list], today_stocks3['return'], test_size=0.1, random_state=42)

X_train4, X_test4, y_train4, y_test4 = train_test_split(today_stocks4[feature_list], today_stocks4['return'], test_size=0.1, random_state=42)

X_train5, X_test5, y_train5, y_test5 = train_test_split(today_stocks5[feature_list], today_stocks5['return'], test_size=0.1, random_state=42)

X_train6, X_test6, y_train6, y_test6 = train_test_split(today_stocks6[feature_list], today_stocks6['return'], test_size=0.1, random_state=42)

X_train7, X_test7, y_train7, y_test7 = train_test_split(today_stocks7[feature_list], today_stocks7['return'], test_size=0.1, random_state=42)

X_train8, X_test8, y_train8, y_test8 = train_test_split(today_stocks8[feature_list], today_stocks8['return'], test_size=0.1, random_state=42)

X_train9, X_test9, y_train9, y_test9 = train_test_split(today_stocks9[feature_list], today_stocks9['return'], test_size=0.1, random_state=42)

param_grid = {'max_depth':list(range(3,7,1))}

params_fit={"eval_metric" : "mae",'eval_set': [[X_test, y_test]],'early_stopping_rounds':10}

params_fit1={"eval_metric" : "mae",'eval_set': [[X_test1, y_test1]],'early_stopping_rounds':10}

params_fit2={"eval_metric" : "mae",'eval_set': [[X_test2, y_test2]],'early_stopping_rounds':10}

params_fit3={"eval_metric" : "mae",'eval_set': [[X_test3, y_test3]],'early_stopping_rounds':10}

params_fit4={"eval_metric" : "mae",'eval_set': [[X_test4, y_test4]],'early_stopping_rounds':10}

params_fit5={"eval_metric" : "mae",'eval_set': [[X_test5, y_test5]],'early_stopping_rounds':10}

params_fit6={"eval_metric" : "mae",'eval_set': [[X_test6, y_test6]],'early_stopping_rounds':10}

params_fit7={"eval_metric" : "mae",'eval_set': [[X_test7, y_test7]],'early_stopping_rounds':10}

params_fit8={"eval_metric" : "mae",'eval_set': [[X_test8, y_test8]],'early_stopping_rounds':10}

params_fit9={"eval_metric" : "mae",'eval_set': [[X_test9, y_test9]],'early_stopping_rounds':10}

gbm = xgb.XGBRegressor(colsample_bylevel=1, colsample_bynode=1, colsample_bytree=.75, gamma=0,learning_rate=0.05, max_delta_step=0,
             missing=-99999, n_estimators=300, random_state=0,reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
             silent=None, subsample=.5, verbosity=1)

search = GridSearchCV(gbm,param_grid=param_grid, verbose=1)

search.fit(X_train,y_train,**params_fit)
print("Best parameter (CV score=%0.3f):" % search.best_score_)
print(search.best_params_)

search1 = GridSearchCV(gbm,param_grid=param_grid, verbose=1)

search1.fit(X_train1,y_train1,**params_fit)
print("Best parameter (CV score=%0.3f):" % search1.best_score_)
print(search.best_params_)

search2 = GridSearchCV(gbm,param_grid=param_grid, verbose=1)

search2.fit(X_train2,y_train2,**params_fit)
print("Best parameter (CV score=%0.3f):" % search2.best_score_)
print(search2.best_params_)

search3 = GridSearchCV(gbm,param_grid=param_grid, verbose=1)

search3.fit(X_train3,y_train3,**params_fit)
print("Best parameter (CV score=%0.3f):" % search3.best_score_)
print(search3.best_params_)

search4 = GridSearchCV(gbm,param_grid=param_grid, verbose=1)

search4.fit(X_train4,y_train4,**params_fit)
print("Best parameter (CV score=%0.3f):" % search4.best_score_)
print(search4.best_params_)

search5 = GridSearchCV(gbm,param_grid=param_grid, verbose=1)

search5.fit(X_train5,y_train5,**params_fit)
print("Best parameter (CV score=%0.3f):" % search5.best_score_)
print(search5.best_params_)

search6 = GridSearchCV(gbm,param_grid=param_grid, verbose=1)

search6.fit(X_train6,y_train6,**params_fit)
print("Best parameter (CV score=%0.3f):" % search6.best_score_)
print(search6.best_params_)

search7 = GridSearchCV(gbm,param_grid=param_grid, verbose=1)

search7.fit(X_train7,y_train7,**params_fit)
print("Best parameter (CV score=%0.3f):" % search7.best_score_)
print(search7.best_params_)

search8 = GridSearchCV(gbm,param_grid=param_grid, verbose=1)

search8.fit(X_train8,y_train8,**params_fit)
print("Best parameter (CV score=%0.3f):" % search8.best_score_)
print(search8.best_params_)

search9 = GridSearchCV(gbm,param_grid=param_grid, verbose=1)

search9.fit(X_train9,y_train9,**params_fit)
print("Best parameter (CV score=%0.3f):" % search9.best_score_)
print(search9.best_params_)

#best model was max depth 3 and we have that estimator fit on whole training data (excluding the early stopping piece)
search.best_estimator_

search1.best_estimator_

search2.best_estimator_

search3.best_estimator_

search4.best_estimator_

search5.best_estimator_

search6.best_estimator_

search7.best_estimator_

search8.best_estimator_

search9.best_estimator_

search.best_estimator_

search.best_estimator_.feature_importances_

search1.best_estimator_.feature_importances_

search2.best_estimator_.feature_importances_

search3.best_estimator_.feature_importances_

search4.best_estimator_.feature_importances_

search5.best_estimator_.feature_importances_

search6.best_estimator_.feature_importances_

search7.best_estimator_.feature_importances_

search8.best_estimator_.feature_importances_

search9.best_estimator_.feature_importances_

# load JS visualization code to notebook
shap.initjs()

print(this_date1)

#input for only this stock
this_data=all_stocks[np.logical_and(all_stocks['Date']==this_date,all_stocks['Symbol']==this_stock)][feature_list]
this_actual=all_stocks[np.logical_and(all_stocks['Date']==this_date,all_stocks['Symbol']==this_stock)]['return']
search.best_estimator_.predict(this_data), this_actual

this_data1=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-30',all_stocks['Symbol']==this_stock)][feature_list]
this_actual1=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-30',all_stocks['Symbol']==this_stock)]['return']
search1.best_estimator_.predict(this_data1), this_actual1

this_data2=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-29',all_stocks['Symbol']==this_stock)][feature_list]
this_actual2=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-29',all_stocks['Symbol']==this_stock)]['return']
search2.best_estimator_.predict(this_data2), this_actual2

this_data3=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-28',all_stocks['Symbol']==this_stock)][feature_list]
this_actual3=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-28',all_stocks['Symbol']==this_stock)]['return']
search3.best_estimator_.predict(this_data3), this_actual1

this_data4=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-25',all_stocks['Symbol']==this_stock)][feature_list]
this_actual4=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-25',all_stocks['Symbol']==this_stock)]['return']
search4.best_estimator_.predict(this_data4), this_actual4

this_data5=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-23',all_stocks['Symbol']==this_stock)][feature_list]
this_actual5=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-23',all_stocks['Symbol']==this_stock)]['return']
search5.best_estimator_.predict(this_data5), this_actual5

this_data6=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-22',all_stocks['Symbol']==this_stock)][feature_list]
this_actual6=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-22',all_stocks['Symbol']==this_stock)]['return']
search6.best_estimator_.predict(this_data6), this_actual6

this_data7=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-21',all_stocks['Symbol']==this_stock)][feature_list]
this_actual7=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-21',all_stocks['Symbol']==this_stock)]['return']
search7.best_estimator_.predict(this_data7), this_actual7

this_data8=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-18',all_stocks['Symbol']==this_stock)][feature_list]
this_actual8=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-18',all_stocks['Symbol']==this_stock)]['return']
search8.best_estimator_.predict(this_data8), this_actual8

this_data9=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-17',all_stocks['Symbol']==this_stock)][feature_list]
this_actual9=all_stocks[np.logical_and(all_stocks['Date']=='2022-11-17',all_stocks['Symbol']==this_stock)]['return']
search9.best_estimator_.predict(this_data9), this_actual9

explainer = shap.TreeExplainer(search.best_estimator_)
shap_values = explainer.shap_values(this_data)

explainer1 = shap.TreeExplainer(search1.best_estimator_)
shap_values1 = explainer1.shap_values(this_data1)

explainer2 = shap.TreeExplainer(search2.best_estimator_)
shap_values2 = explainer2.shap_values(this_data2)

explainer3 = shap.TreeExplainer(search3.best_estimator_)
shap_values3 = explainer3.shap_values(this_data3)

explainer4 = shap.TreeExplainer(search4.best_estimator_)
shap_values4 = explainer4.shap_values(this_data4)

explainer5 = shap.TreeExplainer(search5.best_estimator_)
shap_values5 = explainer5.shap_values(this_data5)

explainer6 = shap.TreeExplainer(search6.best_estimator_)
shap_values6 = explainer6.shap_values(this_data6)

explainer7 = shap.TreeExplainer(search7.best_estimator_)
shap_values7 = explainer7.shap_values(this_data7)

explainer8 = shap.TreeExplainer(search8.best_estimator_)
shap_values8 = explainer8.shap_values(this_data8)

explainer9 = shap.TreeExplainer(search9.best_estimator_)
shap_values9 = explainer9.shap_values(this_data9)

# 
shap.initjs()
#visualize the prediction's explanation
shap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:])

# 
shap.initjs()
#visualize the prediction's explanation
shap.force_plot(explainer1.expected_value, shap_values[0,:], X_test.iloc[0,:])

# 
shap.initjs()
#visualize the prediction's explanation
shap.force_plot(explainer2.expected_value, shap_values[0,:], X_test.iloc[0,:])

# 
shap.initjs()
#visualize the prediction's explanation
shap.force_plot(explainer3.expected_value, shap_values[0,:], X_test.iloc[0,:])

# 
shap.initjs()
#visualize the prediction's explanation
shap.force_plot(explainer4.expected_value, shap_values[0,:], X_test.iloc[0,:])

# 
shap.initjs()
#visualize the prediction's explanation
shap.force_plot(explainer5.expected_value, shap_values[0,:], X_test.iloc[0,:])

# 
shap.initjs()
#visualize the prediction's explanation
shap.force_plot(explainer6.expected_value, shap_values[0,:], X_test.iloc[0,:])

# 
shap.initjs()
#visualize the prediction's explanation
shap.force_plot(explainer7.expected_value, shap_values[0,:], X_test.iloc[0,:])

# 
shap.initjs()
#visualize the prediction's explanation
shap.force_plot(explainer8.expected_value, shap_values[0,:], X_test.iloc[0,:])

# 
shap.initjs()
#visualize the prediction's explanation
shap.force_plot(explainer9.expected_value, shap_values[0,:], X_test.iloc[0,:])

#the baseline value
explainer.expected_value

explainer1.expected_value

explainer2.expected_value

explainer3.expected_value

explainer4.expected_value

explainer5.expected_value

explainer6.expected_value

explainer7.expected_value

explainer8.expected_value

explainer9.expected_value

shap_values

shap_values1

shap_values2

shap_values3

shap_values4

shap_values5

shap_values6

shap_values7

shap_values8

shap_values9

"""### For the assignment you will 
* design features 
* create a model each day for the last 10 trading days 
* attribute returns each day to your features
* create a stacked bar chart over time attributing returns to SHAP base,feature attributions, and model error for your selected stock
* create docker image that creates this visualiztion on a website (flask, streamlit, plotly dash or other)
"""

